---
title: Brain Dynamics
authors:
  - name: Alexander Mark Weber
    affiliations:
      - ref: 1
      - ref: 2
    orcid: 0000-0001-7295-0775
    corresponding: true
    email: aweber@bcchr.ca
    roles:
      - writing
    degrees:
      - PhD
      - MSc
affiliations:
  - id: 1
    department: BC Children's Hospital Research Institute
    name: The University of British Columbia
    address: 938 West 28th Avenue
    city: Vancouver
    state: BC
    country: Canada
    postal-code: V5Z 4H4
  - id: 2
    name: The University of British Columbia
    department: Pediatrics
    address: 2329 West Mall
    city: Vancouver
    region: BC
    country: Canada
    postal-code: V6T 1Z4
abstract: |
  An introduction to the idea of brain dynamics in fMRI studies
bibliography: references.bib
filters:
  - authors-block
crossref:
  fig-title: '**Figure**'
  tbl-title: '**Table**'
  title-delim: "**.**"
csl: _extensions/christopherkenny/nature/csl/springer-vancouver.csl
engine: knitr
# knitr:
#   opts_knit:
#     warning: false
#     message: false
#     error: false
# execute:
#   cache: true
#   echo: false
#   warning: false
#   message: false
#   error: false
---

```{r rsetup, include=FALSE}
# options
knitr::opts_chunk$set(
  echo=FALSE,
  message=FALSE,
  warning=FALSE,
  cache=FALSE
  )
set.seed(1234) # reproducible
options(knitr.kable.NA = '') # how kable handles NA
```

# Introduction

A fractal structure, either spatial or temporal, is composed of smaller parts that exhibit the same pattern at every scale [@koch1904;@kochMethodeGeometriqueElementaire1906;@mandelbrotHowLongCoast1967]. Classic spatial examples of fractals in nature are coastlines [@mandelbrotHowLongCoast1967], circulatory system [@jayalalithaFractalModelBlood2008], and brain anatomy [@ansellUnveilingUniversalAspects2024], where at each level of magnification these structures look the same; i.e. *self-similarity*. For a dynamic process, or in the temporal domain, this is known as *scale invariance*, meaning that both rapid and slow processes follow the same pattern [@rileyTutorialIntroductionAdaptive2012]. Scale-free behavior, characterized by the absence of any inherent scale or preferred length, is a signature characteristic of complex systems that can be understood as the collective outcome of numerous interacting components with weak and random connections [@csermelyWeakLinksStabilizers2006]. Physiological systems usually need a mix of randomness and structure to function optimally. <!-- TODO: need a reference here -->
When a system is operating near the critical point, it is expected to exhibit these long range temporal correlations (LRTCs). Therefore, by analyzing the LRTC of a time-series, valuable insights can be found into the underlying mechanisms driving system behavior, such as memory effects, self-organization, and criticality [@beggsCortexCriticalPoint2022].


---

> "Because pink noise lies between white and Brown(ian) noise, it has been proven to bring stability and adaptability into dynamic processes, thus, crucial properties of well-functioning complex systems" [@bakSelforganizedCriticalityExplanation1987] <!-- from 10.3389/fphys.2018.01378 -->

> "as pink noise arises from the interaction of multiple systems and operates over different scales, it has been shown to contribute to system resiliency and structural integrity if individual components were lost or interrupted" [@lipsitzLossComplexityAging1992;@lipsitzDynamicsStabilityPhysiologic2002] <!-- from 10.3389/fphys.2018.01378 -->

See @hermanFractalCharacterizationComplexity2009 for a really good review

> Scale-free dynamics is a hallmark of complexity viewed as an emergent property of biological systems composed of numerous elements with a network of stochastic (typically weak) connections amongst them (Csermely, 2006) from @mukliImpactHealthyAging2018

Another good review is by Werner (2010) linking fractals/Hurst and Criticality @wernerFractalsNervousSystem2010

---

# Fractal Dimension and Hurst Exponent

Non-periodic fluctuations are prevalent in physiological systems, and these irregular patterns can be mathematically modeled using stochastic, chaotic, or noisy chaotic methods. Stochastic models assume that the fluctuations result from a large number of weak influences, while chaotic models conceptualize that strong nonlinear interactions between a few factors shape the fluctuations. Among the various stochastic approaches, 'fractal' models offer the most accurate representation of reality by considering the self-similar nature of physiological fluctuations over different time scales [@ekeFractalCharacterizationComplexity2002]. Fractal structures were first expressed in the late 19th and early 20th century by mathematicians who generated complex geometrical structures with simple objects (e.g. a triangle) by applying a simple rule of transformation in an infinite number of iterative steps @fig-fractal. A use-case for these geometric structures was not fully realized until the 1960s, when Benoit Mandelbrot formalized them as a new form of geometry capable of describing the complex shapes and forms of nature [@mandelbrotHowLongCoast1967]. 

{{< embed notebooks/Figures.ipynb#fig-fractal >}}

Unlike Euclidean structures that can use axioms and rules to describe an object of integer dimensions, fractal structures can only be characterized by recursive algorithms that extend the use of dimension to the non-integer range [@hermanFractalBranchingPattern2001]. Recognizing this difference, Mandelbrot named these complex structures 'fractals', and defined their non-Euclidean dimension as a 'fractal dimension' (FD) --- that is, a non-integer. For the one-dimensional time-series, FD will be between 1 and 2. While a structure such as the Sierpinski triangle is an 'exact fractal', meaning it is assembled from pieces that are an exact replica of the whole, nature is composed of 'statistical fractals', whose self-similarity is found in the power law scaling of the parameters characterizing their structures at different scales of observation @fig-statisticalfractal. Unlike Euclidean structures which can be defined by axioms, fractals can only be characterized by a set of properties that, when present, indicate the structure is indeed fractal (see @sec-properties).

![**A comparison of statistical and exact fractal patterns.** The two basic forms of fractals are demonstrated. Zooming in on tree branches (left), an exact self-similar element cannot be found. Zooming in on an exact fractal (right), exact replica of the whole are found. Photo by author. Branching fractal made in Python. Figure inspired by Taylor (2006) [@taylorPersonalReflectionsJackson2006]](./image/exact_vs_statistical_tree.png){#fig-statisticalfractal}

The Hurst exponent (H) is a statistical measure used to quantify long-range dependence or persistence in time-series data (i.e. LRTCs). First introduced by Harold Hurst in 1951 for hydrological applications [@hurstLongTermStorageCapacity1951], it was Mandelbrot again who helped popularize its use for studying complex shapes and forms of nature [@mandelbrotClasseProcessusStochastiques1965; @mandelbrotHowLongCoast1967]. Since then it has been applied across various fields, including finance, geophysics, ecology, and neuroscience, to characterize complex systems with intricate dynamics [@molzFractionalBrownianMotion1997; @korvinFractalModelsEarth1992; @parkSelfSimilarNetworkTraffic2000; @gravesBriefHistoryLong2017]. H and FD for time-series can be directly converted, as 
$$
H=2–D.
$$ {#eq-HRD}
This means that FD and H are inversely correlated, and that H is a real number between 0 and 1 (non-inclusive; although it can be extended to > 1; see ...). For the rest of this paper, we will be discussing H. <!-- TODO: -->

*Thus, the basic premise behind fractal time-series analysis is that, beneath the seemingly chaotic and unpredictable variations in the signal, there lies a stable underlying mechanism which can be effectively described using the fewest possible parameters (i.e. H or FD).*

## Properties {#sec-properties}

When H = 0.5 (FD = 1.5), the time-series is completely uncorrelated, has no memory, and is like white noise (pure random). It represents the highest level of unpredictability and entropy, but not necessarily the most complex state in a physiological sense. When H < 0.5 (FD > 1.5), the time-series is said to be anti-persistent, exhibiting negative correlations: i.e. if the signal increases at one point, it's likely to decrease at the next point. This is also known as short-term reversal. This tends to make the time-series more predictable and reduces randomness, often simplifying dynamics. When H > 0.5 (FD < 1.5), the time-series exhibits positive long-range correlations, and is more structured. The time-series displays a long-term trend, meaning past states influence future states.
<!-- However, this does not necessarily mean less complexity. In fact, physiological systems often show fractal-like correlations (0.6 < H < 0.9), which represent a balance between order and variability—often seen as optimal complexity. -->
<!-- So, does moving away from H = 0.5 mean less complexity? If H gets too low (H → 0), the system becomes too rigid and predictable (less complex). If H gets too high (H → 1), the system may become too strongly correlated, reducing variability (also less complex). Complexity is often maximized somewhere in between (e.g., 0.6 < H < 0.9 in biological systems). -->

There are four main properties that must be present for a time-series to be characterized by H or FD: 1) self-similarity; 2) power law scaling relationship; 3) scale-invariance; and 4) a scaling range (@fig-fourprop). Each of these will be discussed in turn. 

### Self-similarity

Self-similarity means that pieces of the structure, when enlarged, resemble larger pieces or the whole (@fig-statisticalfractal and @fig-fourprop A-C). Technically speaking, physiological time-series are self-affine, meaning their scaling is anisotropic (i.e. the proportions between enlarged pieces are different in one direction from those in the other). This is because in one direction (time) the proportions between the enlarged pieces is different than in the other (amplitude of signal; e.g. fMRI BOLD) [@ekePhysiologicalTimeSeries2000]. 

### Power law scaling relationship

Power law scaling means that, for a quantitative property, $q$, is measured in quantities of $s$, its value depends on $s$ according to the following scaling relationship: 

$$
q=f(s).
$$ {#eq-qfs}

For non-fractal objects, the estimate of $q$ using progressively smaller units of measure $s$ will converge to a single value as the size of the measurement units approaches zero. On the other hand, fractals exhibit a power law scaling relationship with $s$, whereby the estimated value of $q$ increases without limit as the size of the $s$ decreases. 

$$
q=ps^{\epsilon}
$$ {#eq-powerlawscalingrelationship}

where $p$ is a factor of proportionality (prefactor) and $\epsilon$ is a negative number, the scaling exponent. The value of $\epsilon$ can be determined as the slope of the linear regression fit to the data pairs on the plot of $\log{q}$ versus $\log{s}$:

$$
\log{q} = \log{p} + \epsilon\log{s}
$$ {#eq-logpowerlawscaling}

Data points for exact fractals will line up along perfectly with regression slope, while statistical fractals scatter around it since the two sides of @eq-logpowerlawscaling are equal only in distribution. 

### Scale-invariance

The ratio of two estimates of $q$ measured at two different scales, $s_{1}$ and $s_{2}$, $q_{2}/q_{1}$ depends only on the ratio of scales (relative scale), $s_{2}/s_{1}$, and not directly on the absolute scale, $s_{1}$ or $s_{2}$

$$
q_{2}/q_{1} = ps_2^\epsilon / ps_1^\epsilon = (s_2 / s_1)^\epsilon
$$ {#eq-scaleinv}

For statistical fractals, like those of nature, $s_2/s_1$ may change in a continuous fashion still leaving the validity of @eq-scaleinv unaffected. The scale-invariance of fractals arises from the fact that the geometrical structure only depends on the scaling factor (ratio of scales), and not the absolute scale. As a result, quantitative properties of smaller parts are similar to those of larger parts.

### Scaling range

Natural fractals may only display scale-invariance within a restricted range, as they are finite (either by definition or due to the fact that they must be sampled). The upper cut-off point ($s_{max}$) in @eq-scalerange, falls within the size range of the structure itself. Similarly, the lower cut-off point ($s_{min}$) falls within the dimensions of the smallest structural elements. The scaling range (SR) is defined in decades

$$
\text{SR} = \log_{10}(s_{max}/s_{min})
$$ {#eq-scalerange}

{{< embed notebooks/Figures.ipynb#fig-fourprop >}}

## Gaussian noise vs Brownian motion

Time-series that are scale-invariant come in two categories: fractional Gaussian noise (fGn) and fractional Brownian motion (fBm) @fig-typicalsamplepaths. fGn signals are stationary, meaning they tend to center themselves along a mean value over time. In contrast, fBm signals are non-stationary, and their values tend to wander away from the mean. fGn and fBm can easily be converted from one to the other: fGn to fBm by applying a successive summation between elements of the fGn series; and fBm to fGn by applying successive differences between elements of a fBm series.

<!-- TODO: find where to put this: -->
This introduces the first method of measuring H of a time-series: using a log-log plot of its power spectral density (@fig-fourprop D and E), calculating the linear-regression slope, and, depending on the slope angle, using one of two equations:

$$
H =
\begin{cases}
\frac{\beta + 1}{2}, & \text{if } -1 < \beta < 1 \\
\frac{\beta - 1}{2}, & \text{if } 1 < \beta < 3
\end{cases}
$$ {#eq-psdH}


{{< embed notebooks/Figures.ipynb#fig-typicalsamplepaths >}}

If the class of signal is not appropriately identified --- for example, when $\beta$ is $\sim$ 1 --- it is possible to seriously miscalculate the H value, such that a true H $\sim$ 0.9 will be calculated as 0.1. Therefore, algorithms and research in properly classifying fractal signals is crucially important.

## Prerequisites to measuring H

### Time-points

Before determining if a time-series exhibits a power-law scaling relationship, one should first determine if the time-series has enough time-points to do so. The rule of thumb is that the power law relationship should be present in a range larger than two decades in the frequency domain of a PSD [@ekePhysiologicalTimeSeries2000]. Due to the Whittaker–Nyquist-Shannon sampling theorem [@shannonCommunicationPresenceNoise1949], one requires two times as many time-points as frequency samples. Since two decades in the frequency domain is 100 distinct frequencies, one would required at least 200 time-points to proceed.

### Power-law scaling relationship

The next step would be to perform a periodogram or PSD and test for a power-law scaling relationship. Usually power laws are tested using a probability distribution [@clausetPowerLawDistributionsEmpirical2009]. However, as the PSD is not a probability distribution, alternative methods must be used. A goodness-of-fit test can be devised based on @clausetPowerLawDistributionsEmpirical2009, which can also help us derive the SR (i.e. $freq_{min}$ and $freq_{max}$). First, a PSD is produced, and the variance ($\sigma^2$), $\beta$, and H are computed using @eq-psdH. Next, the Kolmogorov–Smirnov statistic [@kolmogorovSullaDeterminazioneEmpirica1933; @smirnovTableEstimatingGoodness1948] is used to measure the distance D between the raw PSD data-points and the best-fit linear-regression line used to calculate H (i.e. the residuals). The D in this case is the largest residual error. Then, 1,000 time series of either fGn or fBm (depending on what value of $\beta$ was found; see @eq-psdH) with the same length (n > 200), $\sigma^2$, and Hurst exponent are generated using one of several methods for producing exact fractal signals: spectral synthesis method (SSM) [@peitgenScienceFractalImages1988], Davies-Harte method (DH) [@daviesTestsHurstEffect1987], Cholesky method [@asmussenStochasticSimulationView1998], or the Hosking's method [@hoskingModelingPersistenceHydrological1984]. Each synthetic time-series is converted to a PSD, and the Kolmogorov–Smirnov statistic is used to measure the distance D between the raw PSD data-points and the best-fit linear-regression line. The $p$-value is defined as the fraction of synthetic time series with Ds that are larger than the original D of the original time-series. The larger the $p$-value, the more plausible the synthetic model (either fGn or fBm) is for representing the original time-series, and the better the fit of the original data to a scale-free distribution. The null-hypothesis that the time-series is not scale free is rejected if $p$ < 0.05 (i.e. if $p$ > 0.05, we say that the time-series is scale-free).

However, this method can be computationally intensive, and prone to false negatives, especially signals with very high or low H values. One solution is, instead of applying this test to every voxel in a 4D fMRI brain scan, to segment the brain into separate ROIs (e.g. anatomically based on an atlas, or functionally by first running ICA to identify RSNs), average the time-series within each ROI, which will improve SNR, and then run the power-law scaling test.

A sample python code is provide in @sec-powerlawscalingcode

### Scaling range



### fGn or fBm

### Choosing the right method


> Ideally therefore, a long memory process signal should be categorized as either stationary (fGn) or non-stationary (fBm) and the variance and correlation defined.
from [@ekePhysiologicalTimeSeries2000]

## Methods of measuring H

> Fractal methods are diverse, but their approaches have one thing in common in that they employ equation (2) in fitting their proposed model to data pairs of log feature versus log scale for finding the scaling exponent, ε, from the regression slope.
> Each fractal analysis tool has different performance, prerequisite conditions, and limitations, and each needs thorough evaluation in order to avoid bias or misinterpretation of the derived fractal parameters [5, 6, 8, 9], especially when applied to physiological signals which may be contaminated with noise [15, 28].
from [@ekePhysiologicalTimeSeries2000]

## Neuroscience Applications

H has emerged as a valuable tool in neuroscience and clinical research. Typically, H values reported in adult brains are above 0.5, with higher H values in grey matter than white matter or cerebrospinal fluid [@dongHurstExponentAnalysis2018; @winkMonofractalMultifractalDynamics2008]. Some key findings from neuroscience research include: a decrease in H during task performance [@ciuciuInterplayFunctionalConnectivity2014; @heScaleFreePropertiesFunctional2011]; negative correlations with task novelty and difficulty [@churchillSuppressionScalefreeFMRI2016]; increases with age in the frontal and parietal lobes [@dongHurstExponentAnalysis2018], and hippocampus [@winkAgeCholinergicEffects2006]; decreases with age in the insula, and limbic, occipital and temporal lobes [@dongHurstExponentAnalysis2018]; H < 0.5 in preterm infants [@mellaTemporalComplexityBOLDsignal2024]; and more [@campbellMonofractalAnalysisFunctional2022]. In terms of clinical findings, abnormal H values have been identified in Alzheimer's disease (AD) [@maximFractionalGaussianNoise2005; @warsiCorrelatingBrainBlood2012], autism spectrum disorder (ASD) [@donaTemporalFractalAnalysis2017; @laiShiftRandomnessBrain2010; @linkeAlteredDevelopmentHurst2024; @uscatescuUsingExcitationInhibition2022], mild traumatic brain injury [@donaFractalAnalysisBrain2017], major depressive disorder [@weiIdentifyingMajorDepressive2013; @jingIdentifyingCurrentRemitted2017] and schizophrenia [@sokunbiNonlinearComplexityAnalysis2014; @uscatescuUsingExcitationInhibition2022].

## fMRI preprocessing considerations

### Nuissance regression

When attempting to regress out non-BOLD signal, it is important to apply the regression at the same time, and not in succession. Even performing a band-pass filter after nuissance regression can re-introduce noise components [@lindquistModularPreprocessingPipelines2019].

### Detrending

see @tanabeComparisonDetrendingMethods2002

# Hurst Reviews

### Nonparametric trend estimation in the presence of fractal noise: Application to fMRI time-series analysis - Afshinpour et al. (2008) [@afshinpourNonparametricTrendEstimation2008]

> In this paper, a method for estimating trend in the presence of fractal noise is proposed and applied to fMRI time-series. To this end, a partly linear model (PLM) is fitted to each time-series. The parametric and nonparametric parts of PLM are considered as contributions of hemodynamic response and trend, respectively. Using the whitening property of wavelet transform, the unknown components of the model are estimated in the wavelet domain. The results of the proposed method are compared to those of other parametric trend-removal approaches such as spline and polynomial models. It is shown that the proposed method improves activation detection and decreases variance of the estimated parameters relative to the other methods.

**Notes:**

- trend estimation paper
- 1.5T, 3.9x3.9x6mm, 1.648s TR, 256 time-points
- Hurst method: Wavelet db4 with 5 scales

### Fractal Analysis of BOLD time-series in a Network Associated With Waiting Impulsivity - Akhrif et al. (2018) [@akhrifFractalAnalysisBOLD2018]

> examined **103 ** healthy male students at **rest** and while performing the 5-choice serial reaction time **task**. We addressed fractality in a network associated with waiting impulsivity using the **adaptive fractal analysis (AFA)** approach to determine H. We revealed the fractal nature of the impulsivity network. Furthermore, fractality was influenced by individual impulsivity in terms of decreasing fractality (H) with higher impulsivity in regions of top-down control (left middle frontal gyrus) as well as reward processing (nucleus accumbens and anterior cingulate cortex).

**Notes:**

- fMRI split into low and high frequency components. LFC is the second order polynomial that is a smooth and global fit of the original time course.
- AFA: variance of fluctuation computed around, in this case, a second order polynomial trend $v(i)$ fitted to time-series within each segment $w$, and its size:
$$
F(w) = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \big(u(i) - v(i)\big)^2} \sim w^H
$$ {#eq-AFA}

$N$: length of the time-series

$$
w = 2n + 1, n = 5,6..., 13
$$

H is determined as the slope of the log-log plot log~2~($F(w)$) as a function of log~2~($w$) 

Example code:

```{python}
#| echo: true
#| code-fold: true
#| code-summary: "Code"
import numpy as np

def adaptive_fractal_analysis(signal, n_values=range(5, 14)):
    """
    Perform Adaptive Fractal Analysis (AFA) to compute the Hurst exponent.
    
    Parameters:
        signal (array-like): time-series data to analyze.
        n_values (iterable): Sequence of `n` values to define window sizes as w = 2n + 1.
    
    Returns:
        float: Estimated Hurst exponent (H).
    """
    # Define window sizes as w = 2n + 1
    window_sizes = [2 * n + 1 for n in n_values]
    fluctuations = []

    for window_size in window_sizes:
        segment_variances = []
        for start in range(0, len(signal) - window_size + 1, window_size):
            # Extract the window
            window = signal[start:start + window_size]
            # Fit a second-order polynomial (quadratic fit) and compute residual
            x = np.arange(len(window))
            p = np.polyfit(x, window, deg=2)  # Degree 2 polynomial
            residual = window - np.polyval(p, x)
            # Compute variance of the residuals
            variance = np.var(residual)
            segment_variances.append(variance)
        
        # Compute average variance for this window size
        fluctuations.append(np.mean(segment_variances))

    # Fit the scaling law: log(fluctuations) vs. log(window_sizes)
    log_window_sizes = np.log(window_sizes)
    log_fluctuations = np.log(fluctuations)
    slope, intercept = np.polyfit(log_window_sizes, log_fluctuations, deg=1)
    
    # The slope corresponds to the Hurst exponent
    return slope
```

### Endogenous human brain dynamics recover slowly following cognitive effort - Barnes et al. (2009) [@barnesEndogenousHumanBrain2009]

> 1) Does performance of a cognitively effortful task significantly change fractal scaling properties of fMRI time-series compared to their values before task performance? 2) If so, can we relate the extent of task-related perturbation to the difficulty of the task?
> This result supports the model that endogenous low frequency oscillatory dynamics are relevant to the brain’s response to exogenous stimulation. Moreover, it suggests that large-scale neurocognitive systems measured using fMRI, like the heart and other physiological systems subjected to external demands for enhanced performance, can take a considerable period of time to return to a stable baseline state.

**Notes:**

- maximum likelihood in the wavelet domain

### Wavelets and functional magnetic resonance imaging of the human brain - Bullmore et al. (2004) [@bullmoreWaveletsFunctionalMagnetic2004]

> We provide a brief formal introduction to key properties of the DWT and review the growing literature on its application to fMRI. We focus on three applications in particular: (i) wavelet coefficient resampling or “wavestrapping” of 1-D time-series, 2- to 3-D spatial maps and 4-D spatiotemporal processes; (ii) wavelet-based estimators for signal and noise parameters of time-series regression models assuming the errors are fractional Gaussian noise (fGn); and (iii) wavelet shrinkage in frequentist and Bayesian frameworks to support multiresolution hypothesis testing on spatially extended statistic maps.

**Notes:**

- This paper suggests that motion correction translates many fBm signals to fGn... however, it is not clear where this data comes from.

### Fractal-Based Analysis of fMRI BOLD Signal During Naturalistic Viewing Conditions - Campbell et al. (2021) [@campbellFractalBasedAnalysisFMRI2022]

> We performed fractal analysis on Human Connectome Project 7T fMRI data (n = 72, 41 females, mean age 29.46 ± 3.76 years) to compare H across movie-watching and rest. Results: In contrast to previous work using conventional tasks, we found higher H values for movie relative to rest (mean difference = 0.014; p = 5.279 × 10−7; 95% CI [0.009, 0.019]). H was significantly higher in movie than rest in the visual, somatomotor and dorsal attention networks, but was significantly lower during movie in the frontoparietal and default networks. We found no cross-condition differences in test-retest reliability of H. Finally, we found that H of movie-derived stimulus properties (e.g., luminance changes) were fractal whereas H of head motion estimates were non-fractal.

**Notes:**

- 

### Scale-free brain dynamics under physical and psychological distress: Pre-treatment effects in women diagnosed with breast cancer - Churchill et al. (2015) [@churchillScalefreeBrainDynamics2015]

> In a BOLD functional magnetic resonance imaging study, we scanned three groups during a working memory task: women scheduled to receive chemotherapy or radiotherapy and aged-matched controls. Surprisingly, patients’ BOLD signal exhibited greater H with increasing intensity of anticipated treatment. However, an analysis of H and functional connectivity against self-reported measures of psychological distress (Worry, Anxiety, Depression) and physical distress (Fatigue, Sleep problems) revealed significant interactions. The modulation of (Worry, Anxiety) versus (Fatigue, Sleep Problems, Depression) showed the strongest effect, where higher worry and lower fatigue was related to reduced H in regions involved in visuospatial search, attention, and memory processing. This is also linked to decreased functional connectivity in these brain regions.

**Notes:**

### The suppression of scale-free fMRI brain dynamics across three different sources of effort: Aging, task novelty and task difficulty - Churchill et al. (2016) [@churchillSuppressionScalefreeFMRI2016]

> Decreases in the Hurst exponent (H), which quantifies scale-free signal, was related to three different sources of cognitive effort/task engagement: 1) task difficulty, 2) task novelty, and 3) aging effects. These results were consistently observed across multiple datasets and task paradigms. We also demonstrated that estimates of H are robust across a range of time-window sizes. H was also compared to alternative metrics of BOLD variability (SDBOLD) and global connectivity (Gconn), with effort-related decreases in H producing similar decreases in SDBOLD and Gconn.

**Notes:**

### Interplay between functional connectivity and scale-free dynamics in intrinsic fMRI networks - Ciuciu et al. (2014) [@ciuciuInterplayFunctionalConnectivity2014]

> We applied this framework to fMRI data acquired from healthy young adults at rest and performing a visual detection task. First, we found that scale-invariance existed beyond univariate dynamics, being present also in bivariate cross-temporal dynamics. Second, we observed that frequencies within the scale-free range do not contribute evenly to interregional connectivity, with a systematically stronger contribution of the lowest frequencies, both at rest and during task. Third, in addition to a decrease of the Hurst exponent and inter-regional correlations, task performance modified cross-temporal dynamics, inducing a larger contribution of the highest frequencies within the scale-free range to global correlation. Lastly, we found that across individuals, a weaker task modulation of the frequency contribution to inter-regional connectivity was associated with better task performance manifesting as shorter and less variable reaction times. These findings bring together two related fields that have hitherto been studied separately – resting-state networks and scale-free dynamics, and show that scale-free dynamics of human brain activity manifest in cross-regional interactions as well.

**Notes:**

### Temporal fractal analysis of the rs-BOLD signal identifies brain abnormalities in autism spectrum disorder - Dona et al. (2017) [@donaTemporalFractalAnalysis2017]

> "It is important to mention here that fractal dimension estimation based on adispersional analysis isquite robust with respect to uncorrelated noise and does not require preprocessing"

**Notes:**

- ASD = reduced FD = increased H; 
- rare study to properly define fGn vs fBm first?

### Fractal analysis of brain blood oxygenation level dependent (BOLD) signals from children with mild traumatic brain injury (mTBI) - Dona et al. (2017) [@donaFractalAnalysisBrain2017]

**Notes:**

- children with mTBI; mTBI = reduced FD = increased H
- rare study to properly define fGn vs fBm first?

### Hurst Exponent Analysis of Resting-State fMRI Signal Complexity across the Adult Lifespan - Dong et al. (2018) [@dongHurstExponentAnalysis2018]

> Region-wise and voxel-wise analyses were performed to investigate the effects of age, gender, and their interaction on complexity. In region-wise analysis, we found that the healthy aging is accompanied by a loss of complexity in frontal and parietal lobe and increased complexity in insula, limbic, and temporal lobe. Meanwhile, differences in HE between genders were found to be significant in parietal lobe (p = 0.04, corrected). However, there was no interaction between gender and age. In voxel-wise analysis, the significant complexity decrease with aging was found in frontal and parietal lobe, and complexity increase was found in insula, limbic lobe, occipital lobe, and temporal lobe with aging. Meanwhile, differences in HE between genders were found to be significant in frontal, parietal, and limbic lobe. Furthermore, we found age and sex interaction in right parahippocampal gyrus (p = 0.04, corrected). Our findings reveal HE variations of the rs-fMRI signal across the human adult lifespan and show that HE may serve as a new parameter to assess healthy aging process.

**Notes:**

- They state that increase in age = decrease in complexity

### Pitfalls in fractal time-series analysis: fMRI BOLD as an exemplary case - Eke et al. (2012) [@ekePitfallsFractalTime2012]

>

### Wavelet-Generalized Least Squares: A New BLU Estimator of Linear Regression Models with 1/f Errors - Fadili & Bullmore (2002) [@fadiliWaveletGeneralizedLeastSquares2002]

> 

### Not in one metric: Neuroticism modulates different resting state metrics within distinctive brain regions - Gentili et al. (2017) [@gentiliNotOneMetric2017]

> Metrics more related to the measurement of regional intrinsic brain activity (fALFF, ALFF and REHO), or that provide a parsimonious index of integrated and segregated brain activity (HE), were more broadly modulated in regions related to emotions and their regulation. Metrics related to connectivity were modulated across a wider network of areas. Overall, these results show that neuroticism affects distinct aspects of brain resting state activity.

**Notes:**

- "parsimonious index of integrated and segregated brain activity (HE)"
- HE was inversely correlated to neuroticism

### Proneness to social anxiety modulates neural complexity in the absence of exposure: A resting state fMRI study using Hurst exponent - Gentili et al. (2015) [@gentiliPronenessSocialAnxiety2015]

> Results from fALFF were highly consistent with those obtained using LSAS and BFNE to predict HE. Overall our data indicate that spontaneous brain activity is influenced by the degree of social anxiety, on a continuum and in the absence of social stimuli. These findings suggest that social anxiety is a trait characteristic that shapes brain activity and predisposes to different reactions in social contexts.

**Notes:**

- "A recent article (Rubin et al., 2013) analyzes the robustness of different algorithms with respect to possible fMRI artifacts and time-series lengths. In particular, the relevance of preprocessing steps as motion correction, detrending and filtering were evaluated both on simulated and real fMRI data, while other preprocessing steps like segmentation were not evaluated, although they may have an impact on"
- "The HE of fMRI time-series is generally higher in gray matter than in white matter (Maxim et al., 2005), augments in the hippocampus with aging, and decreases with cholinergic transmission enhancement (Wink et al., 2006)"
- "As pointed out by Maxim (Maxim et al., 2005), fMRI noise, after these pre-processing steps, can be described as fGn."
- "The HE of fMRI time-series is generally higher in gray matter than in white matter (Maxim et al., 2005), augments in the hippocampus with aging, and decreases with cholinergic transmission enhancement (Wink et al., 2006)."
- "a reduction of HE has been observed in autistic and schizophrenic patients (Lai et al., 2010; Sokunbi et al., 2014)"
- "Since the HE can describe long and short range memory dynamics, it has been proposed as a measure of online information-processing efficiency: higher HEs are related to long memory dynamics and to higher temporal redundancy and less freedom to vary (He, 2011)."

### Real-time fractal signal processing in the time domain. - Hartmann et al. (2013) [@hartmannRealtimeFractalSignal2013]

> Here we introduce real-time variants of the Detrended Fluctuation Analysis (DFA) and the closely related Signal Summation Conversion (SSC) methods, which are suitable to estimate the fractal exponent in one pass. 

### Altered fractal dynamics of gait: Reduced stride-interval correlations with aging and Huntington's disease. - Hausdoff et al. (1997) [@hausdorffAlteredFractalDynamics1997]

**Notes:**

- Gait... not fMRI

### Scale-Free Properties of the Functional Magnetic Resonance Imaging Signal during Rest and Task - He (2011) [@heScaleFreePropertiesFunctional2011]

>  its power-law exponent differentiates between brain networks and correlates with fMRI signal variance and brain glucose metabolism. Importantly, in parallel to brain electrical field potentials, the variance and power-law exponent of the fMRI signal decrease during task activation, suggesting that the signal contains more long-range memory during rest and conversely is more efficient at online information processing during task. 
>  The scale-free properties of the fMRI signal and brain electrical field potentials bespeak their respective stationarity and nonstationarity. This suggests that neurovascular coupling mechanism is likely to contain a transformation from nonstationarity to stationarity.

> The fMRI signal time course from each ROI was extracted for each subject and fMRI run. The normalized or non-normalized power spectrum of the fMRI signal was computed using the Bartlett smoothing procedure of deriving the power spectral function from the lagged autocorrelation or auto-covariance function, respectively (Jenkins and Watts, 1998). A Tukey window of 20 fMRI frame width was applied for additional smoothing. The power spectra were then averaged across runs and subjects and across homologous ROIs, resulting in an average power spectrum for each of 21 brain regions (Fig. 2A). Finally, to obtain the power-law exponent β, the <0.1 Hz range of each average power spectrum was fit with a power-law function: P(f) ∝ 1/fβ using a least-squares fit. Using the low-frequency range to fit the power-law exponent avoids aliasing artifact in higher-frequency range (we used TR of 2.16 s, hence Nyquist limit is 0.23 Hz) and yields reliable measurement of the scale-free distribution (Eke et al., 2002).

> The DFA method has the particular advantage of being applicable to both stationary and nonstationary data.
> To analyze our fMRI data, window lengths of 5, 10, 19, 38, and 95 fMRI frames were chosen so that the number of frames in each run (190 after discarding the first four frames) is an integer multiple of the window length.

### Fractal characterization of complexity in dynamic signals: Application to cerebral hemodynamics - Herman (2009) [@hermanFractalCharacterizationComplexity2009]

### Identification of brain activity from fMRI data: Comparison of three fractal scaling analyses. - Hu (2006) [@huIdentificationBrainActivity2006]

### A shift to randomness of brain oscillations in people with autism. Lai (2010) [@laiShiftRandomnessBrain2010]

> Complex fractal scaling of fMRI time-series was found in both groups but globally there was a significant shift to randomness in the ASC (mean H = .758, SD = .045) compared with neurotypical volunteers (mean H = .788, SD = .047).

### Extraversion is encoded by scale-free dynamics of default mode network. Lei (2013) [@leiExtraversionEncodedScalefree2013]

### Fractional Gaussian noise, functional MRI and Alzheimer's disease. Maxim (2005) [@maximFractionalGaussianNoise2005]

> we adopted the Davies-Harte algorithm, which is both exact and fast, to generate the fGn simulations used here. For each value of H = 0.1, ... 0.9, we simulated 1000 realizations of fGn with 512 time-points in each series; we set $\omega^{2} = 1$ for all simulations.

**NOTES:**

- This paper has the figure showing signal goes from fBm to fGn with proper motion regression

### Decomposing multifractal crossovers. Nagy (2017) [@nagyDecomposingMultifractalCrossovers2017]

> The NIRS and fMRI-BOLD low-frequency fluctuations were dominated by a multifractal component over an underlying biologically relevant random noise, thus forming a bimodal signal. The crossover between the EEG signal components was found at the boundary between the δ and θ bands, suggesting an independent generator for the multifractal δ rhythm. The robust implementation of the SFD method should be regarded as essential in the seamless processing of large volumes of bimodal fMRI-BOLD imaging data for the topology of multifractal metrics free of the masking effect of the underlying random noise.

### Optimizing complexity measures for FMRI data: Algorithm, artifact, and sensitivity. Rubin (2013) [@rubinOptimizingComplexityMeasures2013]

> Power-spectrum, Higuchi’s fractal dimension, and generalized Hurst exponent based estimates were most successful by all criteria; the poorest-performing measures were wavelet, detrended fluctuation analysis, aggregated variance, and rescaled range.
> Our results clearly demonstrate that decisions regarding choice of algorithm, signal processing, time-series length, and scanner have a significant impact on the reliability and sensitivity of complexity estimates.
> operating on the edge of chaos, complex systems position themselves for optimal responsivity to inputs, as well as ability to maintain homeostatic regulation.
> Daubechies wavelet based computations (Hdb*) have long computation times, are not sensitive to spikes, and show poor sensitivity to activation, tissue type, and emotional content; for these Daubechies wavelet based estimates the overall performance increases with the wavelet order up to a point (Hdb8), and then deteriorates. HRS and HAV, performed poorly across the board. In terms of image contrast, overlap with activation, and group differences, HDFA* performed poorly as well, with HDFA-S outperforming HDFA and HDFA-L, suggesting that the bulk of useful information is found at shorter lags.
> The most consistently successful measures were the powerspectrum based measures HFFT and HpWelch, with the latter slightly outperforming the former while taking much longer to compute
> Second, it appears that detrending, regressing out the global mean, and excluding low frequencies improves agreement between complexity and activation.
> 300-600 time-points
> Finally, the best measures to use are either the power-spectrum based ones (HFFT or HpWelch) on a restricted frequency range (above ,0.01 Hz),

### Mutual information identifies spurious Hurst phenomena in resting state EEG and fMRI data. von Wegner (2018) [@vonwegnerMutualInformationIdentifies2018]

>  In these processes, which do not have long-range memory by construction, a spurious Hurst phenomenon occurs due to slow relaxation times and heteroscedasticity (time-varying conditional variance). In summary, we find that mutual information correctly distinguishes long-range from short-range dependence in the theoretical and experimental cases discussed. Our results also suggest that the stationary fGn process is not sufficient to describe neural data, which seem to belong to a more general class of stochastic processes, in which multiscale variance effects produce Hurst phenomena without long-range dependence. In our experimental data, the Hurst phenomenon and long-range memory appear as different system properties that should be estimated and interpreted independently.

{{< pagebreak >}}

# References

::: {#refs}
:::

{{< pagebreak >}}

# Appendix

## Python code

### Sample python code for testing power-law scaling {#sec-powerlawscalingcode}

```python
print("HELLO")
```
