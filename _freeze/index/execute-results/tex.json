{
  "hash": "40787a7a761207b141ccdbc29229197f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Brain Dynamics\nauthors:\n  - name: Alexander Mark Weber\n    affiliations:\n      - ref: 1\n      - ref: 2\n    orcid: 0000-0001-7295-0775\n    corresponding: true\n    email: aweber@bcchr.ca\n    roles:\n      - writing\n    degrees:\n      - PhD\n      - MSc\naffiliations:\n  - id: 1\n    department: BC Children's Hospital Research Institute\n    name: The University of British Columbia\n    address: 938 West 28th Avenue\n    city: Vancouver\n    state: BC\n    country: Canada\n    postal-code: V5Z 4H4\n  - id: 2\n    name: The University of British Columbia\n    department: Pediatrics\n    address: 2329 West Mall\n    city: Vancouver\n    region: BC\n    country: Canada\n    postal-code: V6T 1Z4\nabstract: |\n  An introduction to the idea of brain dynamics in fMRI studies\nbibliography: references.bib\nfilters:\n  - authors-block\ncrossref:\n  fig-title: '**Figure**'\n  tbl-title: '**Table**'\n  title-delim: \"**.**\"\ncsl: _extensions/christopherkenny/nature/csl/springer-vancouver.csl\nengine: knitr\n---\n\n\n\n\n\n# Introduction\n\nLike many physiological data, neurological signals --- such as fMRI-BOLD --- appear as irregular time-series. Often thought of as being random or noisy, there was an initial tendency to measure them statistically, such as calculating their standard deviation, assuming that the elements of the series were independent. However, this approach is inadequate when events exhibit interdependence, whether through short- or long-range temporal correlations (LRTCs) [@ekePhysiologicalTimeSeries2000]. Therefore, there is a need for methods that can both identify and characterize random noise from irregular but structured and correlated noise. One such method used for LRTCs is the fractal method.\n\nA fractal structure, either spatial or temporal, is composed of smaller parts that exhibit the same pattern at every scale [@koch1904;@kochMethodeGeometriqueElementaire1906;@mandelbrotHowLongCoast1967]. Classic spatial examples of fractals in nature are coastlines [@mandelbrotHowLongCoast1967], circulatory system [@jayalalithaFractalModelBlood2008], and brain anatomy [@ansellUnveilingUniversalAspects2024], where at each level of magnification these structures look the same; i.e. *self-similarity*. For a dynamic process, or in the temporal domain, this is known as *scale invariance*, meaning that both rapid and slow processes follow the same pattern [@rileyTutorialIntroductionAdaptive2012]. These scale-free LRTCs exhibit a gradual decay in their autocorrelation function, indicating that the system maintains a long-lasting, multiscale memory of its past states, which play a role in shaping its present behaviour. Scale-free behavior is a signature characteristic of complex systems that can be understood as the collective outcome of numerous interacting components with weak and random connections [@csermelyWeakLinksStabilizers2006]. \nPhysiological systems, such as the brain, require a mix of randomness and structure to function optimally [@bakSelforganizedCriticalityExplanation1987]. In many cases, this balance between order and randomness allows the system to remain adaptable and responsive to changing conditions [@lipsitzLossComplexityAging1992;@lipsitzDynamicsStabilityPhysiologic2002]. This adaptability is particularly evident when a system is operating near what is called a 'critical point', the phase change between order and disorder. When a system is operating near the critical point, it is expected to exhibit LRTCs. Therefore, by analyzing the LRTC of a time-series, valuable insights can be found into the underlying mechanisms driving system behavior, such as memory effects, self-organization, and criticality [@beggsCortexCriticalPoint2022].\n\n---\n\n> \"Because pink noise lies between white and Brown(ian) noise, it has been proven to bring stability and adaptability into dynamic processes, thus, crucial properties of well-functioning complex systems\" [@bakSelforganizedCriticalityExplanation1987] <!-- from 10.3389/fphys.2018.01378 -->\n\n> \"as pink noise arises from the interaction of multiple systems and operates over different scales, it has been shown to contribute to system resiliency and structural integrity if individual components were lost or interrupted\" [@lipsitzLossComplexityAging1992;@lipsitzDynamicsStabilityPhysiologic2002] <!-- from 10.3389/fphys.2018.01378 -->\n\nSee @hermanFractalCharacterizationComplexity2009 for a really good review\n\nAnother good review is by Werner (2010) linking fractals/Hurst and Criticality @wernerFractalsNervousSystem2010\n\n> \"This 1/f power relationship implies that perturbations occurring at slow frequencies can cause a cascade of energy dissipation at higher frequencies so that widespread slow oscillations, observed with BOLD contrast fMRI, are modulating faster local events [12].\" from @barnesEndogenousHumanBrain2009\n\n> \"Systems operating at a phase transition are metastable with respect to a set of control parameters, and capable of rapid qualitative change in response to external stimuli.\" \n> Moreover, these transitions occur over short and long spatial and temporal scales (Freeman, 2003) lending credence to the proposition that the brain is maintained in a state of self-organised criticality (Bak et al, 1987).\n> Whilst these models are incomplete in their description of the cortex, they draw interesting parallels with the scale-invariant, small-world topologies observed from resting state fMRI and MEG data (Achard et al, 2006; Achard et al, 2008). From these observations and the results presented in this study, self-organised criticality appears to offer a tractable association between the spatial and temporal behaviours of the brain.\n> from @sucklingEndogenousMultifractalBrain2008\n\n---\n\n# Fractal Dimension and Hurst Exponent\n\nNon-periodic fluctuations are prevalent in physiological systems, and these irregular patterns can be mathematically modeled using stochastic, chaotic, or noisy chaotic methods. Stochastic models assume that the fluctuations result from a large number of weak influences, while chaotic models conceptualize that strong nonlinear interactions between a few factors shape the fluctuations. Among the various stochastic approaches, 'fractal' models offer the most accurate representation of reality by considering the self-similar nature of physiological fluctuations over different time scales [@ekeFractalCharacterizationComplexity2002]. Fractal structures were first expressed in the late 19th and early 20th century by mathematicians who generated complex geometrical structures with simple objects (e.g. a triangle) by applying a simple rule of transformation in an infinite number of iterative steps @fig-fractal. A use-case for these geometric structures was not fully realized until the 1960s, when Benoit Mandelbrot formalized them as a new form of geometry capable of describing the complex shapes and forms of nature [@mandelbrotHowLongCoast1967]. \n\n\n\n{{< embed notebooks/Figures.ipynb#fig-fractal >}}\n\n\n\n\n\nUnlike Euclidean structures that can use axioms and rules to describe an object of integer dimensions, fractal structures can only be characterized by recursive algorithms that extend the use of dimension to the non-integer range [@hermanFractalBranchingPattern2001]. Recognizing this difference, Mandelbrot named these complex structures 'fractals', and defined their non-Euclidean dimension as a 'fractal dimension' (FD) --- that is, a non-integer. For the one-dimensional time-series, FD will be between 1 and 2. While a structure such as the Sierpinski triangle is an 'exact fractal', meaning it is assembled from pieces that are an exact replica of the whole, nature is composed of 'statistical fractals', whose self-similarity is found in the power law scaling of the parameters characterizing their structures at different scales of observation @fig-statisticalfractal. Unlike Euclidean structures which can be defined by axioms, fractals can only be characterized by a set of properties that, when present, indicate the structure is indeed fractal (see @sec-properties).\n\n![**A comparison of statistical and exact fractal patterns.** The two basic forms of fractals are demonstrated. Zooming in on tree branches (left), an exact self-similar element cannot be found. Zooming in on an exact fractal (right), exact replica of the whole are found. Photo by author. Branching fractal made in Python. Figure inspired by Taylor (2006) [@taylorPersonalReflectionsJackson2006]](./image/exact_vs_statistical_tree.png){#fig-statisticalfractal}\n\nThe Hurst exponent (H) is a statistical measure used to quantify long-range dependence or persistence in time-series data (i.e. LRTCs). First introduced by Harold Hurst in 1951 for hydrological applications [@hurstLongTermStorageCapacity1951], it was Mandelbrot again who helped popularize its use for studying complex shapes and forms of nature [@mandelbrotClasseProcessusStochastiques1965; @mandelbrotHowLongCoast1967]. Since then it has been applied across various fields, including finance, geophysics, ecology, and neuroscience, to characterize complex systems with intricate dynamics [@molzFractionalBrownianMotion1997; @korvinFractalModelsEarth1992; @parkSelfSimilarNetworkTraffic2000; @gravesBriefHistoryLong2017]. H and FD for time-series can be directly converted, as \n$$\nH=2–D.\n$$ {#eq-HRD}\nThis means that FD and H are inversely correlated, and that H is a real number between 0 and 1 (non-inclusive; although it can be extended to > 1; see ...). For the rest of this paper, we will be discussing H. <!-- TODO: -->\n\n*Thus, the basic premise behind fractal time-series analysis is: beneath the seemingly chaotic and unpredictable variations in the signal, there lies a stable underlying mechanism which can be effectively described using only a few possible parameters (i.e. H or FD).*\n\n## Properties {#sec-properties}\n\nWhen H = 0.5 (FD = 1.5), the time-series is completely uncorrelated, has no memory, and is like white noise (pure random). It represents the highest level of unpredictability and entropy, but not necessarily the most complex state in a physiological sense. When H < 0.5 (FD > 1.5), the time-series is said to be anti-persistent, exhibiting negative correlations: i.e. if the signal increases at one point, it's likely to decrease at the next point. This is also known as short-term reversal. This tends to make the time-series more predictable and reduces randomness, often simplifying dynamics. When H > 0.5 (FD < 1.5), the time-series exhibits positive long-range correlations, and is more structured. The time-series displays a long-term trend, meaning past states influence future states.\n<!-- However, this does not necessarily mean less complexity. In fact, physiological systems often show fractal-like correlations (0.6 < H < 0.9), which represent a balance between order and variability—often seen as optimal complexity. -->\n<!-- So, does moving away from H = 0.5 mean less complexity? If H gets too low (H → 0), the system becomes too rigid and predictable (less complex). If H gets too high (H → 1), the system may become too strongly correlated, reducing variability (also less complex). Complexity is often maximized somewhere in between (e.g., 0.6 < H < 0.9 in biological systems). -->\n\nThere are four main properties that must be present for a time-series to be characterized by H or FD: 1) self-similarity; 2) power law scaling relationship; 3) scale-invariance; and 4) a scaling range (@fig-fourprop). Each of these will be discussed in turn. \n\n### Self-similarity\n\nSelf-similarity means that pieces of the structure, when enlarged, resemble larger pieces or the whole (@fig-statisticalfractal and @fig-fourprop A-C). Technically speaking, physiological time-series are self-affine, meaning their scaling is anisotropic (i.e. the proportions between enlarged pieces are different in one direction from those in the other). This is because in one direction (time) the proportions between the enlarged pieces is different than in the other (amplitude of signal; e.g. fMRI BOLD) [@ekePhysiologicalTimeSeries2000]. \n\n### Power law scaling relationship\n\nPower law scaling means that, for a quantitative property, $q$, is measured in quantities of $s$, its value depends on $s$ according to the following scaling relationship: \n\n$$\nq=f(s).\n$$ {#eq-qfs}\n\nFor non-fractal objects, the estimate of $q$ using progressively smaller units of measure $s$ will converge to a single value as the size of the measurement units approaches zero. On the other hand, fractals exhibit a power law scaling relationship with $s$, whereby the estimated value of $q$ increases without limit as the size of the $s$ decreases. \n\n$$\nq=ps^{\\epsilon}\n$$ {#eq-powerlawscalingrelationship}\n\nwhere $p$ is a factor of proportionality (prefactor) and $\\epsilon$ is a negative number, the scaling exponent. The value of $\\epsilon$ can be determined as the slope of the linear regression fit to the data pairs on the plot of $\\log{q}$ versus $\\log{s}$:\n\n$$\n\\log{q} = \\log{p} + \\epsilon\\log{s}\n$$ {#eq-logpowerlawscaling}\n\nData points for exact fractals will line up along perfectly with a linear-regression slope, while statistical fractals will scatter around it since the two sides of @eq-logpowerlawscaling are equal only in distribution. \n\n### Scale-invariance\n\nThe ratio of two estimates of $q$ measured at two different scales, $s_{1}$ and $s_{2}$, $q_{2}/q_{1}$ depends only on the ratio of scales (relative scale), $s_{2}/s_{1}$, and not directly on the absolute scale, $s_{1}$ or $s_{2}$\n\n$$\nq_{2}/q_{1} = ps_2^\\epsilon / ps_1^\\epsilon = (s_2 / s_1)^\\epsilon\n$$ {#eq-scaleinv}\n\nFor statistical fractals, like those of nature, $s_2/s_1$ may change in a continuous fashion still leaving the validity of @eq-scaleinv unaffected. The scale-invariance of fractals arises from the fact that the geometrical structure only depends on the scaling factor (ratio of scales), and not the absolute scale. As a result, quantitative properties of smaller parts are similar to those of larger parts.\n\n### Scaling range\n\nNatural fractals may only display scale-invariance within a restricted range, as they are finite (either by definition or due to the fact that they must be sampled). The upper cut-off point ($s_{max}$) in @eq-scalerange, falls within the size range of the structure itself. Similarly, the lower cut-off point ($s_{min}$) falls within the dimensions of the smallest structural elements. The scaling range (SR) is defined in decades\n\n$$\n\\text{SR} = \\log_{10}(s_{max}/s_{min})\n$$ {#eq-scalerange}\n\n\n\n{{< embed notebooks/Figures.ipynb#fig-fourprop >}}\n\n\n\n\n\n\n## Prerequisites to measuring H\n\n### Time-points\n\nBefore determining if a time-series exhibits a power-law scaling relationship, one should first determine if the time-series has enough time-points to do so. The rule of thumb is that the power law relationship should be present in a range larger than two decades in the frequency domain of a PSD [@ekePhysiologicalTimeSeries2000]. Due to the Whittaker–Nyquist-Shannon sampling theorem [@shannonCommunicationPresenceNoise1949], one requires two times as many time-points as frequency samples. Since two decades in the frequency domain is 100 distinct frequencies, one would required at least 200 fMRI volumes / time-points to proceed (not including non-steady state volumes). Furthermore, many algorithms for measuring H require powers of 2, bringing the recommended number of volumes to 256. With a TR of 1s, this translates to $\\sim$ 5 minutes (and 10 minutes for the more common TR of 2s).\n\n### Power-law scaling relationship\n\nThe next step would be to perform a periodogram or PSD and test for a power-law scaling relationship. Usually power laws are tested using a probability distribution [@clausetPowerLawDistributionsEmpirical2009]. However, as the PSD is not a probability distribution, alternative methods must be used. A goodness-of-fit test can be devised based on @clausetPowerLawDistributionsEmpirical2009, which can also help us derive the SR (i.e. $freq_{min}$ and $freq_{max}$). First, a PSD is produced, and the variance ($\\sigma^2$), $\\beta$, and H are computed using @eq-psdH. Next, the Kolmogorov–Smirnov (KS) statistic [@kolmogorovSullaDeterminazioneEmpirica1933; @smirnovTableEstimatingGoodness1948] is used to measure the distance D between the raw PSD data-points and the best-fit linear-regression line used to calculate H (i.e. the residuals). The D in this case is the largest residual error. Then, 1,000 time series of either fGn or fBm (depending on what value of $\\beta$ was found; see @eq-psdH) with the same length, $\\sigma^2$, and Hurst exponent are generated using a simulated exact fractal signal (spectral synthesis method (SSM) [@peitgenScienceFractalImages1988], Davies-Harte method (DH) [@daviesTestsHurstEffect1987], Cholesky method [@asmussenStochasticSimulationView1998], the Hosking's method [@hoskingModelingPersistenceHydrological1984] or the ARFIMA simulation method [@roumeBiasesSimulationAnalysis2019; @grangerINTRODUCTIONLONGMEMORYTIME1980]). The 1,000 synthetic time-series are then converted to PSDs, and the KS distance is again measured. The $p$-value is defined as the fraction of synthetic time series with Ds that are larger than the D of the original time-series. The larger the $p$-value, the more plausible the synthetic model (either fGn or fBm) is for representing the original time-series, and the better the fit of the original data to a scale-free distribution. The null-hypothesis that the time-series is not scale free is rejected if $p$ < 0.05 (i.e. if $p$ > 0.05, we say that the time-series is scale-free).\n\nIt is important to note, however, that the $p$-value in question may need to be adjusted based on the number of time-points.\n<!-- TODO: lots to add here... Such as test 256, 512, and 1024 --> \n\nA sample python code is provide in @sec-powerlawscalingcode\n\n### Scaling range\n\nAs previously mentioned, statistical fractals are unlikely to display their power-law scaling relationship across all scales. Therefore, it is important to test the scaling relationship across a range of scales, determining the minimum and maximum scale for which the scaling relationship exists (if at all). One way to determine this range is to apply the power-law scaling test across a range of scales, starting with the full range, and progressively reducing the size while trying to maintain the widest range. \n\nHowever, this method can be computationally intensive, and prone to false negatives, especially signals with very high or low H values. One solution is, instead of applying this test to every voxel in a 4D fMRI brain scan, to segment the brain into separate ROIs (e.g. anatomically based on an atlas, or functionally by first running ICA to identify RSNs), average the time-series within each ROI, which will improve SNR, and then run the power-law scaling test.\n\nOnce a scaling range is discovered, it is then important to use this range when measuring H. For example, if H is being calculated using a frequency approach, the linear regression can be limited to within the frequency range. However, if another approach is used, a bandpass filter may be necessary first to remove unwanted frequencies. Careful attention should be paid to the type of frequency filtering method employed, and care must be taken not to re-introduce nuissance regressors previously removed [@lindquistModularPreprocessingPipelines2019].\n\n## Classification: Gaussian noise vs Brownian motion\n\nIf the prerequisites above have been met, the next step is to classify the signal type. Fractal signals can be organized into two distinct kinds: fractional Gaussian noise (fGn) and fractional Brownian motion (fBm) @fig-typicalsamplepaths [@mandelbrotFractionalBrownianMotions1968]. fGn signals are stationary, meaning they tend to center along a mean and variance value over time. In contrast, fBm signals are non-stationary: their values tend to wander away from the mean, and their variance is a power function of the time span over which it is computed. Curiously, fGn and fBm can easily be converted from one to the other: fGn to fBm by applying a successive summation between elements of the fGn series; and fBm to fGn by applying successive differences between elements of a fBm series.\n\nWhen calculating the H of a signal, it is imperative to first classify the signal as either fGn or fBm. Failure to do so can result in serious miscalculation of H, as different estimators can produce extremely biased calculations depending on the assumed type of signal. The distinction between fGn and fBm can be seen in their spectral properties. Applying a Fourier transform of a signal to convert it from the time-domain to the frequency domain, we can obtain a power spectral density, which relates the amplitude of the frequency (y-axis) to the frequency (x-axis). The PSD of fGn signals follow a power-law form: $|A(f)|^2 \\propto f^{-\\beta}$, where the exponent $\\beta$ ranges between -1 and 1. This is known as $1/f$ noise, as the power is inversely proportional to frequency. fBm, being a cumulative sum of fGn, has a different spectral behaviour: $|A(f)|^2 \\propto f^{-(\\beta + 2)}$. This steeper decay ($\\beta + 2$) causes fBm to exhibit stronger low-frequency dominance, making it distinct from $1/f$ noise. Thus, when $\\beta \\sim 1$, a signal is said to exist on the $1/f$ boundary.\n\nOne method of calculating H, known as the spectral method or periodogram method (PM), is based on the PSD. The PM method involves creating a log-log plot of the time-series' periodogram using a fast Fourier transform (FFT; @fig-fourprop D and E), calculating the linear-regression slope ($-\\beta$), and, depending on the slope angle, using one of two equations:\n\n$$\nH =\n\\begin{cases}\n\\frac{\\beta + 1}{2}, & \\text{if } -1 < \\beta < 1 \\\\\n\\frac{\\beta - 1}{2}, & \\text{if } 1 < \\beta < 3\n\\end{cases}\n$$ {#eq-psdH}\n\n\n\n{{< embed notebooks/Figures.ipynb#fig-typicalsamplepaths >}}\n\n\n\n\n\nIf the class of signal is not appropriately identified --- for example, when $\\beta$ is $\\sim$ 1 --- it is possible for a true H $\\sim$ 0.9 to be calculated as 0.1. Therefore, algorithms and research in properly classifying fractal signals is crucially important.\n\nSeveral methods have been proposed for classifying fGn and fBm signals, including ones that make use of the PM described above [@ekePhysiologicalTimeSeries2000; @ekeFractalCharacterizationComplexity2002], detrended fluctuation analysis (DFA) [@pengLongrangeAnticorrelationsNonGaussian1993], autoregressive\nfractionally integrated moving average (ARFIMA) [@grangerINTRODUCTIONLONGMEMORYTIME1980], and wavelet entropy [@ramirezpachecoDistinguishingStationaryNonstationary2012; @ramirez-pachecoClassificationFractalSignals2017]. These methods, however, all struggle to distinguish signals whose PSD $\\beta$ values are $\\sim$ 1 [@ekePhysiologicalTimeSeries2000] (AKA the $1/f$ boundary).\n\n### fMRI Signals\n\nAre fMRI signals expected to be fGn or fBm? \n\n## Measuring H\n\nFractal analysis tools vary in performance, assumptions, and limitations. In order to avoid biasing ones results [@bassingthwaighteEvaluatingRescaledRange1994; @; @; @], it is important to choose the right method carefully, especially when applied to physiological signals which can be contaminated by unrelated noise. The following is a non-exhaustive list of methods, and their relative strengths and weaknesses. Despite their diversity, all approaches employ a version of scale-invariance analysis, by fitting a log feature versus log scale for finding a scaling exponent from a regression slope, and then calculating H from this scaling exponent. These methods, however, can be subdivided into those that \n\n- ARIMA (autoregressive integrated moving average) and ARFIMA (autoregressive fractionally integrated moving- average)\n- Adaptive fractal analysis (https://www.frontiersin.org/journals/physiology/articles/10.3389/fphys.2020.00827/full)\n\n### Time-domain\n\n#### Rescaled range analysis (R/S)\n\nThis is the original method used by Hurst in 1951 when studying hydrology and the long term storage capacity of dams [@hurstLongTermStorageCapacity1951].\n\n@dongHurstExponentAnalysis2018\n\n#### Dispersional analysis (DA)\n[@bassingthwaightePhysiologicalHeterogeneityFractals1988]\n\n@donaTemporalFractalAnalysis2017 @donaFractalAnalysisBrain2017\n\n#### Scaled window variance (SWV)\n\n@donaTemporalFractalAnalysis2017 @donaFractalAnalysisBrain2017\n\n#### Generalized Hurst exponent (GHE)\n\n#### Triangle total areas (TTA)\n\n#### Higuchi method (HM)\n\n#### Central estimation (AM & AV)\n\n#### Detrended fluctuation analysis (DFA)\n\n#### Least squares method via standard deviation (LSSD)\n\nBayesian\n\n#### Least squares method via variance (LSV)\n\n#### Whittle's estimator (WE)\n\n\n### Frequency-domain\n\n#### Periodogram method (PM)\n\n#### Welch technique (PSD~Welch~)\n\n#### Local Whittle (LW)\n\nBayesian?\n\n### Mixture\n\n#### Average wavelet coefficient (AWC) https://journals.aps.org/pre/abstract/10.1103/PhysRevE.58.2779\n\n#### Variance vs level (VVL)\n\n#### Maximum likelihood in the wavelet domain (MLW)\n\n> Each fractal analysis tool has different performance, prerequisite conditions, and limitations, and each needs thorough evaluation in order to avoid bias or misinterpretation of the derived fractal parameters [5, 6, 8, 9], especially when applied to physiological signals which may be contaminated with noise [15, 28].\nfrom [@ekePhysiologicalTimeSeries2000]\n\n## Neuroscience Applications\n\nH has emerged as a valuable tool in neuroscience and clinical research. Typically, H values reported in adult brains are above 0.5, with higher H values in grey matter than white matter or cerebrospinal fluid [@dongHurstExponentAnalysis2018; @winkMonofractalMultifractalDynamics2008]. Some key findings from neuroscience research include: a decrease in H during task performance [@ciuciuInterplayFunctionalConnectivity2014; @heScaleFreePropertiesFunctional2011]; negative correlations with task novelty and difficulty [@churchillSuppressionScalefreeFMRI2016]; increases with age in the frontal and parietal lobes [@dongHurstExponentAnalysis2018], and hippocampus [@winkAgeCholinergicEffects2006]; decreases with age in the insula, and limbic, occipital and temporal lobes [@dongHurstExponentAnalysis2018]; H < 0.5 in preterm infants [@mellaTemporalComplexityBOLDsignal2024]; and more [@campbellMonofractalAnalysisFunctional2022]. In terms of clinical findings, abnormal H values have been identified in Alzheimer's disease (AD) [@maximFractionalGaussianNoise2005; @warsiCorrelatingBrainBlood2012], autism spectrum disorder (ASD) [@donaTemporalFractalAnalysis2017; @laiShiftRandomnessBrain2010; @linkeAlteredDevelopmentHurst2024; @uscatescuUsingExcitationInhibition2022], mild traumatic brain injury [@donaFractalAnalysisBrain2017], major depressive disorder [@weiIdentifyingMajorDepressive2013; @jingIdentifyingCurrentRemitted2017] and schizophrenia [@sokunbiNonlinearComplexityAnalysis2014; @uscatescuUsingExcitationInhibition2022].\n\n## fMRI preprocessing considerations\n\n### Nuissance regression\n\nWhen attempting to regress out non-BOLD signal, it is important to apply the regression at the same time, and not in succession. Even performing a band-pass filter after nuissance regression can re-introduce noise components [@lindquistModularPreprocessingPipelines2019].\n\n### Detrending\n\nsee @tanabeComparisonDetrendingMethods2002\n\n# Hurst Reviews\n\n## Table\n\n| Study | n | Age range | Methods | Volumes | TR (s) | Results |\n|------ | -- | -- |------ |------ | -- | -- |\n| Akhrif et al. (2018) [@akhrifFractalAnalysisBOLD2018] | 103 | 19-28 | AFA | task: 425, resting: 350 | 2 | impulsivity: $\\downarrow$ |\n| Barnes et al. (2009) [@barnesEndogenousHumanBrain2009] | 14 | 21-29 | MLW | 2048 | 1.1 | cognitive effort: $\\downarrow$ H |\n| Campbell et al. (2015) [@campbellFractalBasedAnalysisFMRI2022] | 72 | mean 29 | PSD~Welch~ | 900 | 1 | movie-watching: $\\uparrow$ H in visual, somatosensory, and dorsal attention; $\\downarrow$ frontoparietal and DMN |\n| Churchill et al. (2015) [@churchillScalefreeBrainDynamics2015] | 97 (28 chemo; 37 radiation; 32 HC) | n/a | DFA, Wavelet | 285 | 1.5 | worry: $\\downarrow$ H |\n| Churchill et al. (2016) [@churchillSuppressionScalefreeFMRI2016] | three datasets (98): 19; 49; 30 | 20-82 | DFA, PSD~Welch~ | $\\sim$ 300 | 2 | age, task novelty and difficulty: $\\downarrow$ H |\n| Ciuciu et al. (2014) [@ciuciuInterplayFunctionalConnectivity2014] | 17 | 18-27 | Wavelet | 194 | 2.16 | networks |\n| Dona et al. (2017) [@donaTemporalFractalAnalysis2017] | 71 (56 ASD; 15 HC) | mean 13 | PSD, DA, SWV | 300 | 2 | ASD: $\\uparrow$ H |\n| Dona et al. (2017) [@donaFractalAnalysisBrain2017] | 110 (55 mTBI; 55 HC) | mean 13 | PSD, DA, SWV | 180 | 2 | mTBI: $\\uparrow$ H |\n| Dong et al. (2018) [@dongHurstExponentAnalysis2018] | 116 | 19-85 | RS | 260 | 2.5 | age: $\\uparrow$ H frontal and parietal lobe; $\\downarrow$ H insula, limbic, occipital, temporal lobes |\n| Drayne et al. (2024) [@drayneLongrangeTemporalCorrelation2024] | 98 | preterm | PSD~Welch~ | 100 | 3 | preterm: $\\downarrow$ H; differentiates networks |\n| Erbil et al. (2025) [@erbilScaleFreeDynamicsRestingState2025] | 7 | 21-28 | Wavelet | 1,000; 1,000, 3,000 | 1; 0.6; 0.2 | microstates |\n| Gao et al. (2018) [@gaoTemporalDynamicsSpontaneous2018] | 110 | mean 21 | PSD, Wavelet | 232 | 2 | reappraisal scores: $\\downarrow$ H |\n| Gao et al. (2023) [@gaoTemporalDynamicPatterns2023] | 195 (100; 95) | 18-28 | Wavelet | ? | 2 | rumination: $\\uparrow$ H |\n| Gentili et al. (2017) [@gentiliNotOneMetric2017] | 31 | mean 25 | Wavelet | 512 | 1.64 | neuroticism: $\\downarrow$ | \n| Gentili et al. (2015) [@gentiliPronenessSocialAnxiety2015] | 36 | mean 27 | Wavelet | 450 | 2 | social anxiety: $\\uparrow$ H |\n| Guan et al. (2024) [@guanMultifractalDynamicChanges2025] | 31 HC and 31 ADHD; 34 HC and 34 BP; 42 HC and 42 SCHZ | 21-50 | multifractal DFA | 160 | 1.9 | ADHD, BP, SZ: multifractal reduction in bell-shaped asymmetry |\n| He et al. (2011) [@heScaleFreePropertiesFunctional2011] | 17 | 18-27 | DFA, PSD | 194 | 2.16 | task: $\\downarrow$ H; differentiates networks; brain glucose metabolism and neurovascular coupling |\n| Jager et al. (2023) [@jagerDecreasedLongrangeTemporal2024] | 40 (20 task; 20 no task) | 20-32 | DFA | 512 | 1.13 | motor sequence learning: $\\downarrow$ H |\n| Lai et al. (2010) [@laiShiftRandomnessBrain2010] | 63 (33 ASD; 3- HC) | n/a | Wavelet | 512 | 1.3 | ASD: $\\downarrow$ H |\n| Lei et al. (2013) [@leiExtraversionEncodedScalefree2013] | 17 | 18-29 | Wavelet | 200 | 1.5 | extroversion: $\\downarrow$ H in DMN |\n| Lei et al. (2021) [@leiFadedCriticalDynamics2021] | 75 (16 HMMD; 34 IMMD; 25 HC) | mean $\\sim$ 41 | RS | 240 | 2 | moyamoya disease: $\\downarrow$ H |\n| Linke et al. (2024) [@linkeAlteredDevelopmentHurst2024] | 83 | 1.5-5 | WML | 400 | 0.8 | age of children ASD: $\\downarrow$ H in vmPFC |\n| Maxim et al. (2005) [@maximFractionalGaussianNoise2005] | 21 | n/a | LW, Wornell, MLW | 150 | 2 | AD: $\\uparrow$ H |\n| Mella et al. (2024) [@mellaTemporalComplexityBOLDsignal2024] | 716 | preterm | PSD~Welch~ | 2,300 | 0.392 | preterm: $\\downarrow$ H; H starts < 0.5 at preterm age ; differentiates networks | \n| Omidvarnia et al. (2021) [@omidvarniaAssessmentTemporalComplexity2021] | 100 | 22-35 | PSD, DFA | min 250 | 0.72 | cognitive load: $\\downarrow$ H; H and entropy-based complexity highly correlated; H highest in frontoparietal network and default mode network | \n| Rubin et al. (2013) [@rubinOptimizingComplexityMeasures2013] | 22 | ? | Many | ? | ? | HFFT and PSD~Welch~ outperform other methods |\n| Sokunbi et al. (2014) [@sokunbiNonlinearComplexityAnalysis2014] | 29 (13 SZ; 16 HC) | ? | DA, DFA | ? | ? |  SZ: $\\downarrow$ H |\n| Suckling et al. (2008) [@sucklingEndogenousMultifractalBrain2008] | 22 (11 old; 11 young) | 22 and 65 | MLW | 512 | 1.1 | multifractal reanalysis of [@winkAgeCholinergicEffects2006] |\n| Tetereva et al. (2020) | 23 | mean 23.9 | DFA | 300 | 2 | fear: $\\downarrow$ H then $\\uparrow$ H |\n| Usc\\v atescu et al. (2023) [@uscatescuUsingExcitationInhibition2022] | 124 (55 TD; 30 AT; 39 SZ) | ? |  Wavelet | 947? | 0.475 |  ASD and SZ: $\\downarrow$ H |\n| Varley et al. (2020) [@varleyFractalDimensionCortical2020] | 33 (15 HC; 10 min conscious; 8 veg) | ? | HFD | ? | ? |  Lower consciousness: $\\downarrow$ H |\n| von Wegner et al. (2018) [@vonwegnerMutualInformationIdentifies2018] | ? | ? | Wavelet, DFA | 1500 | 2.08 | multiscale variance effects produce Hurst phenomena without long-range dependence |\n| Warsi et al. (2012) [@warsiCorrelatingBrainBlood2012] | 46 (33 AD; 13 HC) | ? | PSD, RD | 2,400 | 0.25 | AD: $\\uparrow$ H |\n| Weber et al. (2014) [@weberPreliminaryStudyEffects2014] | 14 | 22-38 | Wavelet | 512 | 2 | acute alcohol intoxication: mix of $uparrow$ and $downarrow$ H |\n| Wink et al. (2006) [@winkAgeCholinergicEffects2006] | 22 (11 old; 11 young) | 22 and 65 | MLW | 512 | 1.1 | age: $\\uparrow$ H in bilateral hippocampus; scopolamine: $\\uparrow$ H; faster task: $\\uparrow$ H |\n| Wink et al. (2008) [@winkMonofractalMultifractalDynamics2008] | 11 | mean 35 $\\pm$ 10 | Wavelet | 136 | 1.1 | latency in fame decision task: $\\downarrow$ H |\n| Xie et al. (2024) [@xiePharmacoresistantTemporalLobe2024] | 70 | ? | Wavelet | 700 | 0.6 | pharmaco-resistant TLE: $\\downarrow$ H |\n\n: **fMRI-Hurst studies.** An attempt to gather all published fMRI studies that have used Hurst or Hurst-like analysis, some stats, and the main findings. Main findings are almost certainly more nuanced than how we have reported them here; we have attempted to condense the findings as succinctly as possible. n = number of subjects in the study; TR = repition time; MLWD = maximum likelihood wavelet; PSD~Welch~ = power spectral density Welch method; DMN = default mode network; DFA = detrended fluctuation analysis; DA = dispersional analysis; SWV = scaled window variance; RS = rescaled range; LW = local Whittle;   {#tbl-fmrihurst}\n\n### Nonparametric trend estimation in the presence of fractal noise: Application to fMRI time-series analysis - Afshinpour et al. (2008) [@afshinpourNonparametricTrendEstimation2008]\n\n> In this paper, a method for estimating trend in the presence of fractal noise is proposed and applied to fMRI time-series. To this end, a partly linear model (PLM) is fitted to each time-series. The parametric and nonparametric parts of PLM are considered as contributions of hemodynamic response and trend, respectively. Using the whitening property of wavelet transform, the unknown components of the model are estimated in the wavelet domain. The results of the proposed method are compared to those of other parametric trend-removal approaches such as spline and polynomial models. It is shown that the proposed method improves activation detection and decreases variance of the estimated parameters relative to the other methods.\n\n**Notes:**\n\n- trend estimation paper\n- 1.5T, 3.9x3.9x6mm, 1.648s TR, 256 time-points\n- Hurst method: Wavelet db4 with 5 scales\n\n### Fractal Analysis of BOLD time-series in a Network Associated With Waiting Impulsivity - Akhrif et al. (2018) [@akhrifFractalAnalysisBOLD2018]\n\n> examined **103 ** healthy male students at **rest** and while performing the 5-choice serial reaction time **task**. We addressed fractality in a network associated with waiting impulsivity using the **adaptive fractal analysis (AFA)** approach to determine H. We revealed the fractal nature of the impulsivity network. Furthermore, fractality was influenced by individual impulsivity in terms of decreasing fractality (H) with higher impulsivity in regions of top-down control (left middle frontal gyrus) as well as reward processing (nucleus accumbens and anterior cingulate cortex).\n\n**Notes:**\n\n- fMRI split into low and high frequency components. LFC is the second order polynomial that is a smooth and global fit of the original time course.\n- AFA: variance of fluctuation computed around, in this case, a second order polynomial trend $v(i)$ fitted to time-series within each segment $w$, and its size:\n$$\nF(w) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\big(u(i) - v(i)\\big)^2} \\sim w^H\n$$ {#eq-AFA}\n\n$N$: length of the time-series\n\n$$\nw = 2n + 1, n = 5,6..., 13\n$$\n\nH is determined as the slope of the log-log plot log~2~($F(w)$) as a function of log~2~($w$) \n\nExample code:\n\n````python\n#| echo: true\n#| code-fold: true\n#| code-summary: \"Code\"\nimport numpy as np\n\ndef adaptive_fractal_analysis(signal, n_values=range(5, 14)):\n    \"\"\"\n    Perform Adaptive Fractal Analysis (AFA) to compute the Hurst exponent.\n    \n    Parameters:\n        signal (array-like): time-series data to analyze.\n        n_values (iterable): Sequence of `n` values to define window sizes as w = 2n + 1.\n    \n    Returns:\n        float: Estimated Hurst exponent (H).\n    \"\"\"\n    # Define window sizes as w = 2n + 1\n    window_sizes = [2 * n + 1 for n in n_values]\n    fluctuations = []\n\n    for window_size in window_sizes:\n        segment_variances = []\n        for start in range(0, len(signal) - window_size + 1, window_size):\n            # Extract the window\n            window = signal[start:start + window_size]\n            # Fit a second-order polynomial (quadratic fit) and compute residual\n            x = np.arange(len(window))\n            p = np.polyfit(x, window, deg=2)  # Degree 2 polynomial\n            residual = window - np.polyval(p, x)\n            # Compute variance of the residuals\n            variance = np.var(residual)\n            segment_variances.append(variance)\n        \n        # Compute average variance for this window size\n        fluctuations.append(np.mean(segment_variances))\n\n    # Fit the scaling law: log(fluctuations) vs. log(window_sizes)\n    log_window_sizes = np.log(window_sizes)\n    log_fluctuations = np.log(fluctuations)\n    slope, intercept = np.polyfit(log_window_sizes, log_fluctuations, deg=1)\n    \n    # The slope corresponds to the Hurst exponent\n    return slope\n````\n\n### Endogenous human brain dynamics recover slowly following cognitive effort - Barnes et al. (2009) [@barnesEndogenousHumanBrain2009]\n\n> 1) Does performance of a cognitively effortful task significantly change fractal scaling properties of fMRI time-series compared to their values before task performance? 2) If so, can we relate the extent of task-related perturbation to the difficulty of the task?\n> This result supports the model that endogenous low frequency oscillatory dynamics are relevant to the brain’s response to exogenous stimulation. Moreover, it suggests that large-scale neurocognitive systems measured using fMRI, like the heart and other physiological systems subjected to external demands for enhanced performance, can take a considerable period of time to return to a stable baseline state.\n\n**Notes:**\n\n- maximum likelihood in the wavelet domain\n\n### Wavelets and functional magnetic resonance imaging of the human brain - Bullmore et al. (2004) [@bullmoreWaveletsFunctionalMagnetic2004]\n\n> We provide a brief formal introduction to key properties of the DWT and review the growing literature on its application to fMRI. We focus on three applications in particular: (i) wavelet coefficient resampling or “wavestrapping” of 1-D time-series, 2- to 3-D spatial maps and 4-D spatiotemporal processes; (ii) wavelet-based estimators for signal and noise parameters of time-series regression models assuming the errors are fractional Gaussian noise (fGn); and (iii) wavelet shrinkage in frequentist and Bayesian frameworks to support multiresolution hypothesis testing on spatially extended statistic maps.\n\n**Notes:**\n\n- This paper suggests that motion correction translates many fBm signals to fGn... however, it is not clear where this data comes from.\n\n### Fractal-Based Analysis of fMRI BOLD Signal During Naturalistic Viewing Conditions - Campbell et al. (2021) [@campbellFractalBasedAnalysisFMRI2022]\n\n> We performed fractal analysis on Human Connectome Project 7T fMRI data (n = 72, 41 females, mean age 29.46 ± 3.76 years) to compare H across movie-watching and rest. Results: In contrast to previous work using conventional tasks, we found higher H values for movie relative to rest (mean difference = 0.014; p = 5.279 × 10−7; 95% CI [0.009, 0.019]). H was significantly higher in movie than rest in the visual, somatomotor and dorsal attention networks, but was significantly lower during movie in the frontoparietal and default networks. We found no cross-condition differences in test-retest reliability of H. Finally, we found that H of movie-derived stimulus properties (e.g., luminance changes) were fractal whereas H of head motion estimates were non-fractal.\n\n**Notes:**\n\n- \n\n### Scale-free brain dynamics under physical and psychological distress: Pre-treatment effects in women diagnosed with breast cancer - Churchill et al. (2015) [@churchillScalefreeBrainDynamics2015]\n\n> In a BOLD functional magnetic resonance imaging study, we scanned three groups during a working memory task: women scheduled to receive chemotherapy or radiotherapy and aged-matched controls. Surprisingly, patients’ BOLD signal exhibited greater H with increasing intensity of anticipated treatment. However, an analysis of H and functional connectivity against self-reported measures of psychological distress (Worry, Anxiety, Depression) and physical distress (Fatigue, Sleep problems) revealed significant interactions. The modulation of (Worry, Anxiety) versus (Fatigue, Sleep Problems, Depression) showed the strongest effect, where higher worry and lower fatigue was related to reduced H in regions involved in visuospatial search, attention, and memory processing. This is also linked to decreased functional connectivity in these brain regions.\n\n**Notes:**\n\n### The suppression of scale-free fMRI brain dynamics across three different sources of effort: Aging, task novelty and task difficulty - Churchill et al. (2016) [@churchillSuppressionScalefreeFMRI2016]\n\n> Decreases in the Hurst exponent (H), which quantifies scale-free signal, was related to three different sources of cognitive effort/task engagement: 1) task difficulty, 2) task novelty, and 3) aging effects. These results were consistently observed across multiple datasets and task paradigms. We also demonstrated that estimates of H are robust across a range of time-window sizes. H was also compared to alternative metrics of BOLD variability (SDBOLD) and global connectivity (Gconn), with effort-related decreases in H producing similar decreases in SDBOLD and Gconn.\n\n**Notes:**\n\n### Interplay between functional connectivity and scale-free dynamics in intrinsic fMRI networks - Ciuciu et al. (2014) [@ciuciuInterplayFunctionalConnectivity2014]\n\n> We applied this framework to fMRI data acquired from healthy young adults at rest and performing a visual detection task. First, we found that scale-invariance existed beyond univariate dynamics, being present also in bivariate cross-temporal dynamics. Second, we observed that frequencies within the scale-free range do not contribute evenly to interregional connectivity, with a systematically stronger contribution of the lowest frequencies, both at rest and during task. Third, in addition to a decrease of the Hurst exponent and inter-regional correlations, task performance modified cross-temporal dynamics, inducing a larger contribution of the highest frequencies within the scale-free range to global correlation. Lastly, we found that across individuals, a weaker task modulation of the frequency contribution to inter-regional connectivity was associated with better task performance manifesting as shorter and less variable reaction times. These findings bring together two related fields that have hitherto been studied separately – resting-state networks and scale-free dynamics, and show that scale-free dynamics of human brain activity manifest in cross-regional interactions as well.\n\n**Notes:**\n\n### Temporal fractal analysis of the rs-BOLD signal identifies brain abnormalities in autism spectrum disorder - Dona et al. (2017) [@donaTemporalFractalAnalysis2017]\n\n> \"It is important to mention here that fractal dimension estimation based on adispersional analysis isquite robust with respect to uncorrelated noise and does not require preprocessing\"\n\n**Notes:**\n\n- ASD = reduced FD = increased H; \n- rare study to properly define fGn vs fBm first?\n\n### Fractal analysis of brain blood oxygenation level dependent (BOLD) signals from children with mild traumatic brain injury (mTBI) - Dona et al. (2017) [@donaFractalAnalysisBrain2017]\n\n**Notes:**\n\n- children with mTBI; mTBI = reduced FD = increased H\n- rare study to properly define fGn vs fBm first?\n\n### Hurst Exponent Analysis of Resting-State fMRI Signal Complexity across the Adult Lifespan - Dong et al. (2018) [@dongHurstExponentAnalysis2018]\n\n> Region-wise and voxel-wise analyses were performed to investigate the effects of age, gender, and their interaction on complexity. In region-wise analysis, we found that the healthy aging is accompanied by a loss of complexity in frontal and parietal lobe and increased complexity in insula, limbic, and temporal lobe. Meanwhile, differences in HE between genders were found to be significant in parietal lobe (p = 0.04, corrected). However, there was no interaction between gender and age. In voxel-wise analysis, the significant complexity decrease with aging was found in frontal and parietal lobe, and complexity increase was found in insula, limbic lobe, occipital lobe, and temporal lobe with aging. Meanwhile, differences in HE between genders were found to be significant in frontal, parietal, and limbic lobe. Furthermore, we found age and sex interaction in right parahippocampal gyrus (p = 0.04, corrected). Our findings reveal HE variations of the rs-fMRI signal across the human adult lifespan and show that HE may serve as a new parameter to assess healthy aging process.\n\n**Notes:**\n\n- They state that increase in age = decrease in complexity\n\n### Pitfalls in fractal time-series analysis: fMRI BOLD as an exemplary case - Eke et al. (2012) [@ekePitfallsFractalTime2012]\n\n>\n\n### Wavelet-Generalized Least Squares: A New BLU Estimator of Linear Regression Models with 1/f Errors - Fadili & Bullmore (2002) [@fadiliWaveletGeneralizedLeastSquares2002]\n\n> \n\n### Not in one metric: Neuroticism modulates different resting state metrics within distinctive brain regions - Gentili et al. (2017) [@gentiliNotOneMetric2017]\n\n> Metrics more related to the measurement of regional intrinsic brain activity (fALFF, ALFF and REHO), or that provide a parsimonious index of integrated and segregated brain activity (HE), were more broadly modulated in regions related to emotions and their regulation. Metrics related to connectivity were modulated across a wider network of areas. Overall, these results show that neuroticism affects distinct aspects of brain resting state activity.\n\n**Notes:**\n\n- \"parsimonious index of integrated and segregated brain activity (HE)\"\n- HE was inversely correlated to neuroticism\n\n### Proneness to social anxiety modulates neural complexity in the absence of exposure: A resting state fMRI study using Hurst exponent - Gentili et al. (2015) [@gentiliPronenessSocialAnxiety2015]\n\n> Results from fALFF were highly consistent with those obtained using LSAS and BFNE to predict HE. Overall our data indicate that spontaneous brain activity is influenced by the degree of social anxiety, on a continuum and in the absence of social stimuli. These findings suggest that social anxiety is a trait characteristic that shapes brain activity and predisposes to different reactions in social contexts.\n\n**Notes:**\n\n- \"A recent article (Rubin et al., 2013) analyzes the robustness of different algorithms with respect to possible fMRI artifacts and time-series lengths. In particular, the relevance of preprocessing steps as motion correction, detrending and filtering were evaluated both on simulated and real fMRI data, while other preprocessing steps like segmentation were not evaluated, although they may have an impact on\"\n- \"The HE of fMRI time-series is generally higher in gray matter than in white matter (Maxim et al., 2005), augments in the hippocampus with aging, and decreases with cholinergic transmission enhancement (Wink et al., 2006)\"\n- \"As pointed out by Maxim (Maxim et al., 2005), fMRI noise, after these pre-processing steps, can be described as fGn.\"\n- \"The HE of fMRI time-series is generally higher in gray matter than in white matter (Maxim et al., 2005), augments in the hippocampus with aging, and decreases with cholinergic transmission enhancement (Wink et al., 2006).\"\n- \"a reduction of HE has been observed in autistic and schizophrenic patients (Lai et al., 2010; Sokunbi et al., 2014)\"\n- \"Since the HE can describe long and short range memory dynamics, it has been proposed as a measure of online information-processing efficiency: higher HEs are related to long memory dynamics and to higher temporal redundancy and less freedom to vary (He, 2011).\"\n\n### Real-time fractal signal processing in the time domain. - Hartmann et al. (2013) [@hartmannRealtimeFractalSignal2013]\n\n> Here we introduce real-time variants of the Detrended Fluctuation Analysis (DFA) and the closely related Signal Summation Conversion (SSC) methods, which are suitable to estimate the fractal exponent in one pass. \n\n### Altered fractal dynamics of gait: Reduced stride-interval correlations with aging and Huntington's disease. - Hausdoff et al. (1997) [@hausdorffAlteredFractalDynamics1997]\n\n**Notes:**\n\n- Gait... not fMRI\n\n### Scale-Free Properties of the Functional Magnetic Resonance Imaging Signal during Rest and Task - He (2011) [@heScaleFreePropertiesFunctional2011]\n\n>  its power-law exponent differentiates between brain networks and correlates with fMRI signal variance and brain glucose metabolism. Importantly, in parallel to brain electrical field potentials, the variance and power-law exponent of the fMRI signal decrease during task activation, suggesting that the signal contains more long-range memory during rest and conversely is more efficient at online information processing during task. \n>  The scale-free properties of the fMRI signal and brain electrical field potentials bespeak their respective stationarity and nonstationarity. This suggests that neurovascular coupling mechanism is likely to contain a transformation from nonstationarity to stationarity.\n\n> The fMRI signal time course from each ROI was extracted for each subject and fMRI run. The normalized or non-normalized power spectrum of the fMRI signal was computed using the Bartlett smoothing procedure of deriving the power spectral function from the lagged autocorrelation or auto-covariance function, respectively (Jenkins and Watts, 1998). A Tukey window of 20 fMRI frame width was applied for additional smoothing. The power spectra were then averaged across runs and subjects and across homologous ROIs, resulting in an average power spectrum for each of 21 brain regions (Fig. 2A). Finally, to obtain the power-law exponent β, the <0.1 Hz range of each average power spectrum was fit with a power-law function: P(f) ∝ 1/fβ using a least-squares fit. Using the low-frequency range to fit the power-law exponent avoids aliasing artifact in higher-frequency range (we used TR of 2.16 s, hence Nyquist limit is 0.23 Hz) and yields reliable measurement of the scale-free distribution (Eke et al., 2002).\n\n> The DFA method has the particular advantage of being applicable to both stationary and nonstationary data.\n> To analyze our fMRI data, window lengths of 5, 10, 19, 38, and 95 fMRI frames were chosen so that the number of frames in each run (190 after discarding the first four frames) is an integer multiple of the window length.\n\n### Fractal characterization of complexity in dynamic signals: Application to cerebral hemodynamics - Herman (2009) [@hermanFractalCharacterizationComplexity2009]\n\n### Identification of brain activity from fMRI data: Comparison of three fractal scaling analyses. - Hu (2006) [@huIdentificationBrainActivity2006]\n\n### A shift to randomness of brain oscillations in people with autism. Lai (2010) [@laiShiftRandomnessBrain2010]\n\n> Complex fractal scaling of fMRI time-series was found in both groups but globally there was a significant shift to randomness in the ASC (mean H = .758, SD = .045) compared with neurotypical volunteers (mean H = .788, SD = .047).\n\n### Extraversion is encoded by scale-free dynamics of default mode network. Lei (2013) [@leiExtraversionEncodedScalefree2013]\n\n### Fractional Gaussian noise, functional MRI and Alzheimer's disease. Maxim (2005) [@maximFractionalGaussianNoise2005]\n\n> we adopted the Davies-Harte algorithm, which is both exact and fast, to generate the fGn simulations used here. For each value of H = 0.1, ... 0.9, we simulated 1000 realizations of fGn with 512 time-points in each series; we set $\\omega^{2} = 1$ for all simulations.\n\n**NOTES:**\n\n- This paper has the figure showing signal goes from fBm to fGn with proper motion regression\n\n### Decomposing multifractal crossovers. Nagy (2017) [@nagyDecomposingMultifractalCrossovers2017]\n\n> The NIRS and fMRI-BOLD low-frequency fluctuations were dominated by a multifractal component over an underlying biologically relevant random noise, thus forming a bimodal signal. The crossover between the EEG signal components was found at the boundary between the δ and θ bands, suggesting an independent generator for the multifractal δ rhythm. The robust implementation of the SFD method should be regarded as essential in the seamless processing of large volumes of bimodal fMRI-BOLD imaging data for the topology of multifractal metrics free of the masking effect of the underlying random noise.\n\n### Optimizing complexity measures for FMRI data: Algorithm, artifact, and sensitivity. Rubin (2013) [@rubinOptimizingComplexityMeasures2013]\n\n> Power-spectrum, Higuchi’s fractal dimension, and generalized Hurst exponent based estimates were most successful by all criteria; the poorest-performing measures were wavelet, detrended fluctuation analysis, aggregated variance, and rescaled range.\n> Our results clearly demonstrate that decisions regarding choice of algorithm, signal processing, time-series length, and scanner have a significant impact on the reliability and sensitivity of complexity estimates.\n> operating on the edge of chaos, complex systems position themselves for optimal responsivity to inputs, as well as ability to maintain homeostatic regulation.\n> Daubechies wavelet based computations (Hdb*) have long computation times, are not sensitive to spikes, and show poor sensitivity to activation, tissue type, and emotional content; for these Daubechies wavelet based estimates the overall performance increases with the wavelet order up to a point (Hdb8), and then deteriorates. HRS and HAV, performed poorly across the board. In terms of image contrast, overlap with activation, and group differences, HDFA* performed poorly as well, with HDFA-S outperforming HDFA and HDFA-L, suggesting that the bulk of useful information is found at shorter lags.\n> The most consistently successful measures were the powerspectrum based measures HFFT and HpWelch, with the latter slightly outperforming the former while taking much longer to compute\n> Second, it appears that detrending, regressing out the global mean, and excluding low frequencies improves agreement between complexity and activation.\n> 300-600 time-points\n> Finally, the best measures to use are either the power-spectrum based ones (HFFT or HpWelch) on a restricted frequency range (above ,0.01 Hz),\n\n### Mutual information identifies spurious Hurst phenomena in resting state EEG and fMRI data. von Wegner (2018) [@vonwegnerMutualInformationIdentifies2018]\n\n>  In these processes, which do not have long-range memory by construction, a spurious Hurst phenomenon occurs due to slow relaxation times and heteroscedasticity (time-varying conditional variance). In summary, we find that mutual information correctly distinguishes long-range from short-range dependence in the theoretical and experimental cases discussed. Our results also suggest that the stationary fGn process is not sufficient to describe neural data, which seem to belong to a more general class of stochastic processes, in which multiscale variance effects produce Hurst phenomena without long-range dependence. In our experimental data, the Hurst phenomenon and long-range memory appear as different system properties that should be estimated and interpreted independently.\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n# References\n\n::: {#refs}\n:::\n\n\n\n{{< pagebreak >}}\n\n\n\n\n\n# Appendix\n\n## Python code\n\n### Sample python code for testing power-law scaling {#sec-powerlawscalingcode}\n\n```python\nprint(\"HELLO\")\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}